layers:
  - hidden:
      units: 25
      type: dense
      activation: relu
      init_scale: 0.1
  - hidden:
      units: 25
      type: dense
      activation: relu
      init_scale: 0.1

  - output:
      type: dense
      activation: linear
      init_scale: 0.1